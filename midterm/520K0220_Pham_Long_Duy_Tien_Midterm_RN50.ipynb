{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM64xIQp8IuNr6Ft27UwAL1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a77c963be987446592a737b7cbf2ef69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_071b739287604700a6898238945da25c",
              "IPY_MODEL_fdc0b13b34494e729c5c72ecbc2d1a45",
              "IPY_MODEL_558f621db8a042d9ad6eab6854147750"
            ],
            "layout": "IPY_MODEL_2ec80299f3384d979bf29923ea7b9049"
          }
        },
        "071b739287604700a6898238945da25c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a7b1a2ed4e546858e9fcf726c8eaaa7",
            "placeholder": "​",
            "style": "IPY_MODEL_0c87f3477b574f87b47b0610937f1f96",
            "value": "100%"
          }
        },
        "fdc0b13b34494e729c5c72ecbc2d1a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd5a28752fdb4ea6943d5b88ce114d27",
            "max": 810,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b66d30d380344b1a8d6f25480ae5933",
            "value": 810
          }
        },
        "558f621db8a042d9ad6eab6854147750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a66d061d477b4550838953efd126c09a",
            "placeholder": "​",
            "style": "IPY_MODEL_b0423fd8f09443b697bfe2e076f7f0cc",
            "value": " 810/810 [07:48&lt;00:00,  1.91it/s]"
          }
        },
        "2ec80299f3384d979bf29923ea7b9049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a7b1a2ed4e546858e9fcf726c8eaaa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c87f3477b574f87b47b0610937f1f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd5a28752fdb4ea6943d5b88ce114d27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b66d30d380344b1a8d6f25480ae5933": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a66d061d477b4550838953efd126c09a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0423fd8f09443b697bfe2e076f7f0cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up"
      ],
      "metadata": {
        "id": "tTccfS2WSIUa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SinnrphUF3eY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, preprocess_input\n",
        "from tensorflow.keras.layers import Layer, Conv2D, Input, BatchNormalization, Add, \\\n",
        "AveragePooling2D, MaxPooling2D, ZeroPadding2D, ReLU, Dense, Flatten, Dropout, \\\n",
        "Embedding, Attention, RepeatVector, Concatenate\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import datasets, layers, models, optimizers, regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l2\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using Flickr8K for this."
      ],
      "metadata": {
        "id": "rx7RC5A56Km9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory for the dataset and working\n",
        "!mkdir flickr8k\n",
        "!mkdir working\n",
        "\n",
        "# Download the Flickr8k dataset\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "\n",
        "# Unzip the dataset files into the same directory\n",
        "!unzip Flickr8k_Dataset.zip -d flickr8k/ > /dev/null\n",
        "!unzip Flickr8k_text.zip -d flickr8k/ > /dev/null\n"
      ],
      "metadata": {
        "id": "mLFx2SJIglGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = 'flickr8k'\n",
        "WORKING_DIR = 'working'"
      ],
      "metadata": {
        "id": "8CsdbFbJh40V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HACK** In the captions I have, there is an issue. There's an entry that it should be for the file `2258277193_586949ec62.jpg.1` on `Flickr8k.token.txt`. But the file doesn't exist at all, leading the training to error out due to the lack of the instance of the `2258277193_586949ec62.jpg.1`file."
      ],
      "metadata": {
        "id": "l-FWd_3_4NMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Make a backup of the original caption file\n",
        "shutil.copyfile('flickr8k/Flickr8k.token.txt', 'flickr8k/Flickr8k.token.original.txt')\n",
        "\n",
        "# Read the original caption file\n",
        "with open('flickr8k/Flickr8k.token.txt', 'r') as f:\n",
        "    captions = f.readlines()\n",
        "\n",
        "# Remove the problematic line\n",
        "captions = [c for c in captions if '2258277193_586949ec62.jpg.1' not in c]\n",
        "\n",
        "# Write the new caption file\n",
        "with open('flickr8k/Flickr8k.token.txt', 'w') as f:\n",
        "    f.writelines(captions)\n"
      ],
      "metadata": {
        "id": "O8EkzjBVIJ4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"modifed\")\n",
        "!grep \"2258277193\" \"flickr8k/Flickr8k.token.txt\"\n",
        "print(\"original\")\n",
        "!grep \"2258277193\" \"flickr8k/Flickr8k.token.original.txt\" "
      ],
      "metadata": {
        "id": "BbwmyYyjIUIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet50Revamp\n"
      ],
      "metadata": {
        "id": "6OZ2M378Ufyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load resnetv2 model\n",
        "model = ResNet50V2()\n",
        "# restructure the model\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "# summarize\n",
        "#print(model.summary())"
      ],
      "metadata": {
        "id": "OfonLUd0VBWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features from image\n",
        "features = {}\n",
        "directory = os.path.join(BASE_DIR, 'Flicker8k_Dataset')\n",
        "\n",
        "for img_name in tqdm(os.listdir(directory)):\n",
        "    # load the image from file\n",
        "    img_path = directory + '/' + img_name\n",
        "    image = load_img(img_path, target_size=(224, 224))\n",
        "    # convert image pixels to numpy array\n",
        "    image = img_to_array(image)\n",
        "    # reshape data for model\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    # preprocess image for vgg\n",
        "    image = preprocess_input(image)\n",
        "    # extract features\n",
        "    feature = model.predict(image, verbose=0)\n",
        "    # get image ID\n",
        "    image_id = img_name.split('.')[0]\n",
        "    # store feature\n",
        "    features[image_id] = feature"
      ],
      "metadata": {
        "id": "PtXo4tSjh1yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store features in pickle\n",
        "pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))"
      ],
      "metadata": {
        "id": "wfkrl07skmgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load features from pickle\n",
        "with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n",
        "    features = pickle.load(f)"
      ],
      "metadata": {
        "id": "9AZaBtEIknVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(BASE_DIR, 'Flickr8k.token.txt'), 'r') as f:\n",
        "    captions_doc = f.read()"
      ],
      "metadata": {
        "id": "kFYo6oQekoYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create mapping of image to captions\n",
        "mapping = {}\n",
        "# process lines\n",
        "for line in tqdm(captions_doc.split('\\n')):\n",
        "    # split the line by hashtag/pound\n",
        "    tokens = line.split('#')\n",
        "    if len(line) < 2:\n",
        "        continue\n",
        "    image_id, caption = tokens[0], tokens[1:]\n",
        "    # remove extension from image ID\n",
        "    image_id = image_id.split('.')[0]\n",
        "    # convert caption list to string\n",
        "    caption = \" \".join(caption)\n",
        "    # create list if needed\n",
        "    if image_id not in mapping:\n",
        "        mapping[image_id] = []\n",
        "    # store the caption\n",
        "    mapping[image_id].append(caption)"
      ],
      "metadata": {
        "id": "7W7qEMISnPF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(mapping)"
      ],
      "metadata": {
        "id": "YdggjAHHnSZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features['1000268201_693b08cb0e'].shape"
      ],
      "metadata": {
        "id": "X1WVFZmhnTx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(mapping):\n",
        "    for key, captions in mapping.items():\n",
        "        for i in range(len(captions)):\n",
        "            # take one caption at a time\n",
        "            caption = captions[i]\n",
        "            # preprocessing steps\n",
        "            # convert to lowercase\n",
        "            caption = caption.lower()\n",
        "            # delete digits, special chars, etc., \n",
        "            caption = caption.replace('[^A-Za-z]', '')\n",
        "            # delete additional spaces\n",
        "            caption = caption.replace('\\s+', ' ')\n",
        "            # add start and end tags to the caption\n",
        "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
        "            captions[i] = caption"
      ],
      "metadata": {
        "id": "v1pSaZZ-nV7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# before preprocess of text\n",
        "mapping['1000268201_693b08cb0e']\n",
        "# preprocess the text\n",
        "clean(mapping)\n",
        "# after preprocess of text\n",
        "mapping['1000268201_693b08cb0e']"
      ],
      "metadata": {
        "id": "sassBMC7nW7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_captions = []\n",
        "for key in mapping:\n",
        "    for caption in mapping[key]:\n",
        "        all_captions.append(caption)\n",
        "len(all_captions)\n",
        "all_captions[:10]"
      ],
      "metadata": {
        "id": "NqEpLM7ZsaWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ndGTevzshXp",
        "outputId": "417b20f5-763f-41ed-969d-8beae11e8d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8485"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get maximum length, based on the longest one\n",
        "max_length = max(len(caption.split()) for caption in all_captions)\n",
        "max_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ooc7lXk0s6XV",
        "outputId": "a78d087f-0d8b-4547-ee81-b558075ff3cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_ids = list(mapping.keys())\n",
        "split = int(len(image_ids) * 0.90)\n",
        "train = image_ids[:split]\n",
        "test = image_ids[split:]"
      ],
      "metadata": {
        "id": "lcBDti12shvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data generator to get data in batch (avoids session crash)\n",
        "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
        "    # loop over images\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n = 0\n",
        "    while 1:\n",
        "        for key in data_keys:\n",
        "            n += 1\n",
        "            captions = mapping[key]\n",
        "            # process each caption\n",
        "            for caption in captions:\n",
        "                # encode the sequence\n",
        "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "                # split the sequence into X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    # split into input and output pairs\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    \n",
        "                    # store the sequences\n",
        "                    X1.append(features[key][0])\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "            if n == batch_size:\n",
        "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
        "                yield [X1, X2], y\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n = 0"
      ],
      "metadata": {
        "id": "vqynceu1s0Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image feature layers\n",
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "image_features = RepeatVector(max_length)(fe2)\n",
        "\n",
        "# sequence feature layers\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "seq_features = LSTM(256, return_sequences=True)(se2)\n",
        "\n",
        "# attention mechanism\n",
        "attention = Attention()([image_features, seq_features])\n",
        "context_vector = Concatenate(axis=-1)([seq_features, attention])\n",
        "\n",
        "# decoder model\n",
        "decoder1 = LSTM(256)(context_vector)\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "# tie everything together\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "xXaSA6OEjk9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder model\n",
        "# image feature layers\n",
        "inputs1 = Input(shape=(2048,), name=\"image_input\")\n",
        "fe1 = Dropout(0.4)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "# sequence feature layers\n",
        "inputs2 = Input(shape=(max_length,), name=\"text_input\")\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.4)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "\n",
        "# decoder model\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
      ],
      "metadata": {
        "id": "M4YT09Mds0x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# plot the model\n",
        "plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "ZUEhXLU5lAk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "steps = len(train) // batch_size\n",
        "\n",
        "generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
        "history = model.fit(generator, epochs=epochs, steps_per_epoch=steps, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39px_QXDtjDg",
        "outputId": "f0e3b144-5484-48fd-9377-a32811207461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "227/227 [==============================] - 115s 459ms/step - loss: 5.7953\n",
            "Epoch 2/10\n",
            "227/227 [==============================] - 93s 408ms/step - loss: 4.8681\n",
            "Epoch 3/10\n",
            "227/227 [==============================] - 91s 400ms/step - loss: 4.5162\n",
            "Epoch 4/10\n",
            "227/227 [==============================] - 90s 398ms/step - loss: 4.3129\n",
            "Epoch 5/10\n",
            "227/227 [==============================] - 91s 400ms/step - loss: 4.1662\n",
            "Epoch 6/10\n",
            "227/227 [==============================] - 91s 400ms/step - loss: 4.0421\n",
            "Epoch 7/10\n",
            "227/227 [==============================] - 91s 400ms/step - loss: 3.9381\n",
            "Epoch 8/10\n",
            "227/227 [==============================] - 90s 398ms/step - loss: 3.8465\n",
            "Epoch 9/10\n",
            "227/227 [==============================] - 94s 412ms/step - loss: 3.7653\n",
            "Epoch 10/10\n",
            "227/227 [==============================] - 90s 396ms/step - loss: 3.6943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "model.save(WORKING_DIR+'/model_rn50.h5')"
      ],
      "metadata": {
        "id": "uA372DH_PoGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def idx_to_word(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ],
      "metadata": {
        "id": "2_XSWDKdPokw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate caption for an image\n",
        "def predict_caption(model, image, tokenizer, max_length):\n",
        "    # add start tag for generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the max length of sequence\n",
        "    for i in range(max_length):\n",
        "        # encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad the sequence\n",
        "        sequence = pad_sequences([sequence], max_length)\n",
        "        # predict next word\n",
        "        yhat = model.predict([image, sequence], verbose=0)\n",
        "        # get index with high probability\n",
        "        yhat = np.argmax(yhat)\n",
        "        # convert index to word\n",
        "        word = idx_to_word(yhat, tokenizer)\n",
        "        # stop if word not found\n",
        "        if word is None:\n",
        "            break\n",
        "        # append word as input for generating next word\n",
        "        in_text += \" \" + word\n",
        "        # stop if we reach end tag\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "      \n",
        "    return in_text"
      ],
      "metadata": {
        "id": "QoGLPvMZPqqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu"
      ],
      "metadata": {
        "id": "ivONeF5VPsku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validate with test data\n",
        "actual, predicted = list(), list()\n",
        "\n",
        "for key in tqdm(test):\n",
        "    # get actual caption\n",
        "    captions = mapping[key]\n",
        "    # predict the caption for image\n",
        "    y_pred = predict_caption(model, features[key], tokenizer, max_length) \n",
        "    # split into words\n",
        "    actual_captions = [caption.split() for caption in captions]\n",
        "    y_pred = y_pred.split()\n",
        "    # append to the list\n",
        "    actual.append(actual_captions)\n",
        "    predicted.append(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a77c963be987446592a737b7cbf2ef69",
            "071b739287604700a6898238945da25c",
            "fdc0b13b34494e729c5c72ecbc2d1a45",
            "558f621db8a042d9ad6eab6854147750",
            "2ec80299f3384d979bf29923ea7b9049",
            "9a7b1a2ed4e546858e9fcf726c8eaaa7",
            "0c87f3477b574f87b47b0610937f1f96",
            "fd5a28752fdb4ea6943d5b88ce114d27",
            "3b66d30d380344b1a8d6f25480ae5933",
            "a66d061d477b4550838953efd126c09a",
            "b0423fd8f09443b697bfe2e076f7f0cc"
          ]
        },
        "id": "SwXpCjkGPs4s",
        "outputId": "a4c8f498-0c87-4ca2-a115-14984836a0ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/810 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a77c963be987446592a737b7cbf2ef69"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calcuate BLEU score\n",
        "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "print(\"BLEU-3: %f\" % corpus_bleu(actual, predicted, weights=(0.2, 0.3, 0.5, 0)))\n",
        "print(\"BLEU-4: %f\" % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "metadata": {
        "id": "hWiZj5goPu7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "def generate_caption(image_name):\n",
        "    # load the image\n",
        "    # image_name = \"1001773457_577c3a7d70.jpg\"\n",
        "    image_id = image_name.split('.')[0]\n",
        "    img_path = os.path.join(BASE_DIR, \"Flicker8k_Dataset\", image_name)\n",
        "    image = Image.open(img_path)\n",
        "    captions = mapping[image_id]\n",
        "    print('---------------------Actual---------------------')\n",
        "    for caption in captions:\n",
        "        print(caption)\n",
        "    # predict the caption\n",
        "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
        "    print('--------------------Predicted--------------------')\n",
        "    print(y_pred)\n",
        "    print(\"BLEU-1: %f\" % sentence_bleu(captions, y_pred, weights=(1.0, 0, 0, 0)))\n",
        "    print(\"BLEU-2: %f\" % sentence_bleu(captions, y_pred,  weights=(0.5, 0.5, 0, 0)))\n",
        "    print(\"BLEU-3: %f\" % sentence_bleu(captions, y_pred,  weights=(0.2, 0.3, 0.5, 0)))\n",
        "    print(\"BLEU-4: %f\" % sentence_bleu(captions, y_pred,  weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "    plt.figure()\n",
        "    plt.imshow(image)"
      ],
      "metadata": {
        "id": "TtsYwKgcU16l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_rand_cap():\n",
        "    id = np.random.randint(0,len(mapping))\n",
        "    filepaths = os.listdir(BASE_DIR +\"/Flicker8k_Dataset\")\n",
        "    sample_path = filepaths[id] \n",
        "    generate_caption(sample_path)"
      ],
      "metadata": {
        "id": "gEOKcIx7U3yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_rand_cap()"
      ],
      "metadata": {
        "id": "A_OzfxgjVFPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval"
      ],
      "metadata": {
        "id": "ImdS0bgvrXW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import os\n",
        "\n",
        "def calculate_accuracy(model, features, tokenizer, max_length, num_images=None):\n",
        "    total_bleu_score = 0\n",
        "    count = 0\n",
        "    if num_images is None:\n",
        "        num_images = len(features)\n",
        "    else:\n",
        "        num_images = min(num_images, len(features))\n",
        "\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Load the image\n",
        "        image_name = image_id + \".jpg\"\n",
        "        img_path = os.path.join(BASE_DIR, \"Flicker8k_Dataset\", image_name)\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        # Get the ground truth captions for this image\n",
        "        captions = mapping[image_id]\n",
        "\n",
        "        # Generate the predicted caption\n",
        "        y_pred = predict_caption(model, feature, tokenizer, max_length)\n",
        "\n",
        "        # Calculate the BLEU score for the predicted caption\n",
        "        bleu_score = sentence_bleu(captions, y_pred)\n",
        "        \n",
        "        # Add the BLEU score to the total\n",
        "        total_bleu_score += bleu_score\n",
        "\n",
        "    # Calculate the average BLEU score\n",
        "    average_bleu_score = total_bleu_score / num_images\n",
        "\n",
        "    # Print the final results\n",
        "    print(f\"Average BLEU Score: {average_bleu_score:.4f}\")"
      ],
      "metadata": {
        "id": "v0hvGJxarZql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_accuracy(model, features, tokenizer, max_length, num_images=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_8orWRqrakL",
        "outputId": "891c02eb-2167-433e-a72b-f0c65b3f221e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 1/10\n",
            "Image 2/10\n",
            "Image 3/10\n",
            "Image 4/10\n",
            "Image 5/10\n",
            "Image 6/10\n",
            "Image 7/10\n",
            "Image 8/10\n",
            "Image 9/10\n",
            "Image 10/10\n",
            "Average BLEU Score: 0.5683\n"
          ]
        }
      ]
    }
  ]
}