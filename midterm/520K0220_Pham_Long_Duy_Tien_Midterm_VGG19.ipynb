{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP/j0uXTB2OakXG2vW4ec7W"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up"
      ],
      "metadata": {
        "id": "tTccfS2WSIUa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SinnrphUF3eY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet50V2, preprocess_input\n",
        "from tensorflow.keras.layers import Layer, Conv2D, Input, BatchNormalization, Add, \\\n",
        "AveragePooling2D, MaxPooling2D, ZeroPadding2D, ReLU, Dense, Flatten, Dropout, \\\n",
        "Embedding, Attention, RepeatVector, Concatenate\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import datasets, layers, models, optimizers, regularizers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l2\n",
        "from tqdm.notebook import tqdm\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using Flickr8K for this."
      ],
      "metadata": {
        "id": "rx7RC5A56Km9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory for the dataset and working\n",
        "!mkdir flickr8k\n",
        "!mkdir working\n",
        "\n",
        "# Download the Flickr8k dataset\n",
        "!wget --quiet https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip \n",
        "!wget --quiet https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "\n",
        "# Unzip the dataset files into the same directory\n",
        "!unzip Flickr8k_Dataset.zip -d flickr8k/ > /dev/null\n",
        "!unzip Flickr8k_text.zip -d flickr8k/ > /dev/null\n"
      ],
      "metadata": {
        "id": "mLFx2SJIglGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = 'flickr8k'\n",
        "WORKING_DIR = 'working'"
      ],
      "metadata": {
        "id": "8CsdbFbJh40V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HACK** In the captions I have, there is an issue. There's an entry that it should be for the file `2258277193_586949ec62.jpg.1` on `Flickr8k.token.txt`. But the file doesn't exist at all, leading the training to error out due to the lack of the instance of the `2258277193_586949ec62.jpg.1`file."
      ],
      "metadata": {
        "id": "l-FWd_3_4NMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Make a backup of the original caption file\n",
        "shutil.copyfile('flickr8k/Flickr8k.token.txt', 'flickr8k/Flickr8k.token.original.txt')\n",
        "\n",
        "# Read the original caption file\n",
        "with open('flickr8k/Flickr8k.token.txt', 'r') as f:\n",
        "    captions = f.readlines()\n",
        "\n",
        "# Remove the problematic line\n",
        "captions = [c for c in captions if '2258277193_586949ec62.jpg.1' not in c]\n",
        "\n",
        "# Write the new caption file\n",
        "with open('flickr8k/Flickr8k.token.txt', 'w') as f:\n",
        "    f.writelines(captions)\n"
      ],
      "metadata": {
        "id": "O8EkzjBVIJ4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"modifed\")\n",
        "!grep \"2258277193\" \"flickr8k/Flickr8k.token.txt\"\n",
        "print(\"original\")\n",
        "!grep \"2258277193\" \"flickr8k/Flickr8k.token.original.txt\" "
      ],
      "metadata": {
        "id": "BbwmyYyjIUIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG16\n"
      ],
      "metadata": {
        "id": "uqfMhXDzaNSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input"
      ],
      "metadata": {
        "id": "PKySUpHbaQPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load vgg19 model\n",
        "model = VGG19()\n",
        "# restructure the model\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "# summarize\n",
        "#print(model.summary())"
      ],
      "metadata": {
        "id": "_hYvWL6paNS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features from image\n",
        "features = {}\n",
        "directory = os.path.join(BASE_DIR, 'Flicker8k_Dataset')\n",
        "\n",
        "for img_name in tqdm(os.listdir(directory)):\n",
        "    # load the image from file\n",
        "    img_path = directory + '/' + img_name\n",
        "    image = load_img(img_path, target_size=(224, 224))\n",
        "    # convert image pixels to numpy array\n",
        "    image = img_to_array(image)\n",
        "    # reshape data for model\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    # preprocess image for vgg\n",
        "    image = preprocess_input(image)\n",
        "    # extract features\n",
        "    feature = model.predict(image, verbose=0)\n",
        "    # get image ID\n",
        "    image_id = img_name.split('.')[0]\n",
        "    # store feature\n",
        "    features[image_id] = feature"
      ],
      "metadata": {
        "id": "PGuVF3dvaNS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store features in pickle\n",
        "pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))"
      ],
      "metadata": {
        "id": "q7Bke4EGaNS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load features from pickle\n",
        "with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n",
        "    features = pickle.load(f)"
      ],
      "metadata": {
        "id": "MlT6cgBHaNS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T7CQHRnAaNS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HACK** In the captions I have, there is an issue. There's an entry that it should be for the file `2258277193_586949ec62.jpg.1` on `Flickr8k.token.txt`. But the file doesn't exist at all, leading the training to error out due to the lack of the instance of the `2258277193_586949ec62.jpg.1`file."
      ],
      "metadata": {
        "id": "PXh9kQ69aNS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Make a backup of the original caption file\n",
        "shutil.copyfile('flickr8k/Flickr8k.token.txt', 'flickr8k/Flickr8k.token.original.txt')\n",
        "\n",
        "# Read the original caption file\n",
        "with open('flickr8k/Flickr8k.token.txt', 'r') as f:\n",
        "    captions = f.readlines()\n",
        "\n",
        "# Remove the problematic line\n",
        "captions = [c for c in captions if '2258277193_586949ec62.jpg.1' not in c]\n",
        "\n",
        "# Write the new caption file\n",
        "with open('flickr8k/Flickr8k.token.txt', 'w') as f:\n",
        "    f.writelines(captions)\n"
      ],
      "metadata": {
        "id": "lTuq3Dq-aNS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"modifed\")\n",
        "!grep \"2258277193\" \"flickr8k/Flickr8k.token.txt\"\n",
        "print(\"original\")\n",
        "!grep \"2258277193\" \"flickr8k/Flickr8k.token.original.txt\" "
      ],
      "metadata": {
        "id": "RjZ8xd4raNS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(BASE_DIR, 'Flickr8k.token.txt'), 'r') as f:\n",
        "    captions_doc = f.read()"
      ],
      "metadata": {
        "id": "hDTUEi22aNS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create mapping of image to captions\n",
        "mapping = {}\n",
        "# process lines\n",
        "for line in tqdm(captions_doc.split('\\n')):\n",
        "    # split the line by hashtag/pound\n",
        "    tokens = line.split('#')\n",
        "    if len(line) < 2:\n",
        "        continue\n",
        "    image_id, caption = tokens[0], tokens[1:]\n",
        "    # remove extension from image ID\n",
        "    image_id = image_id.split('.')[0]\n",
        "    # convert caption list to string\n",
        "    caption = \" \".join(caption)\n",
        "    # create list if needed\n",
        "    if image_id not in mapping:\n",
        "        mapping[image_id] = []\n",
        "    # store the caption\n",
        "    mapping[image_id].append(caption)"
      ],
      "metadata": {
        "id": "MTPibi7EaNS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(mapping)"
      ],
      "metadata": {
        "id": "OdjNejxHaNS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features['1000268201_693b08cb0e'].shape"
      ],
      "metadata": {
        "id": "A8lsrjFXaNS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(mapping):\n",
        "    for key, captions in mapping.items():\n",
        "        for i in range(len(captions)):\n",
        "            # take one caption at a time\n",
        "            caption = captions[i]\n",
        "            # preprocessing steps\n",
        "            # convert to lowercase\n",
        "            caption = caption.lower()\n",
        "            # delete digits, special chars, etc., \n",
        "            caption = caption.replace('[^A-Za-z]', '')\n",
        "            # delete additional spaces\n",
        "            caption = caption.replace('\\s+', ' ')\n",
        "            # add start and end tags to the caption\n",
        "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
        "            captions[i] = caption"
      ],
      "metadata": {
        "id": "XLo8y5kYaNS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# before preprocess of text\n",
        "mapping['1000268201_693b08cb0e']\n",
        "# preprocess the text\n",
        "clean(mapping)\n",
        "# after preprocess of text\n",
        "mapping['1000268201_693b08cb0e']"
      ],
      "metadata": {
        "id": "LdConK6SaNTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_captions = []\n",
        "for key in mapping:\n",
        "    for caption in mapping[key]:\n",
        "        all_captions.append(caption)\n",
        "len(all_captions)\n",
        "all_captions[:10]"
      ],
      "metadata": {
        "id": "21Q8AkCzaNTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "IxPAqHKIaNTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get maximum length, based on the longest one\n",
        "max_length = max(len(caption.split()) for caption in all_captions)\n",
        "max_length"
      ],
      "metadata": {
        "id": "d1uBUr5taNTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_ids = list(mapping.keys())\n",
        "split = int(len(image_ids) * 0.90)\n",
        "train = image_ids[:split]\n",
        "test = image_ids[split:]"
      ],
      "metadata": {
        "id": "hoigtZpWaNTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data generator to get data in batch (avoids session crash)\n",
        "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
        "    # loop over images\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n = 0\n",
        "    while 1:\n",
        "        for key in data_keys:\n",
        "            n += 1\n",
        "            captions = mapping[key]\n",
        "            # process each caption\n",
        "            for caption in captions:\n",
        "                # encode the sequence\n",
        "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "                # split the sequence into X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    # split into input and output pairs\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    \n",
        "                    # store the sequences\n",
        "                    X1.append(features[key][0])\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "            if n == batch_size:\n",
        "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
        "                yield [X1, X2], y\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n = 0"
      ],
      "metadata": {
        "id": "0tNpWCcnaNTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# image feature layers\n",
        "inputs1 = Input(shape=(4096,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "image_features = RepeatVector(max_length)(fe2)\n",
        "\n",
        "# sequence feature layers\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "seq_features = LSTM(256, return_sequences=True)(se2)\n",
        "\n",
        "# attention mechanism\n",
        "attention = Attention()([image_features, seq_features])\n",
        "context_vector = Concatenate(axis=-1)([seq_features, attention])\n",
        "\n",
        "# decoder model\n",
        "decoder1 = LSTM(256)(context_vector)\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "# tie everything together\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "pEwy48mQ4euq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the model\n",
        "plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "Idtjva7f4k4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder model\n",
        "# image feature layers\n",
        "inputs1 = Input(shape=(4096,))\n",
        "fe1 = Dropout(0.4)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "# sequence feature layers\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.4)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "\n",
        "# decoder model\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# plot the model\n",
        "plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "Qn8M6xOKaNTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "epochs = 10\n",
        "batch_size = 32\n",
        "steps = len(train) // batch_size\n",
        "\n",
        "for i in range(epochs):\n",
        "    # create data generator\n",
        "    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
        "    # fit for one epoch\n",
        "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
      ],
      "metadata": {
        "id": "55eHF5eJaNTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "model.save(WORKING_DIR+'/model_vgg16.h5')"
      ],
      "metadata": {
        "id": "yf4sAOQ8aNTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def idx_to_word(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ],
      "metadata": {
        "id": "XVNRTEVQaNTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate caption for an image\n",
        "def predict_caption(model, image, tokenizer, max_length):\n",
        "    # add start tag for generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the max length of sequence\n",
        "    for i in range(max_length):\n",
        "        # encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad the sequence\n",
        "        sequence = pad_sequences([sequence], max_length)\n",
        "        # predict next word\n",
        "        yhat = model.predict([image, sequence], verbose=0)\n",
        "        # get index with high probability\n",
        "        yhat = np.argmax(yhat)\n",
        "        # convert index to word\n",
        "        word = idx_to_word(yhat, tokenizer)\n",
        "        # stop if word not found\n",
        "        if word is None:\n",
        "            break\n",
        "        # append word as input for generating next word\n",
        "        in_text += \" \" + word\n",
        "        # stop if we reach end tag\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "      \n",
        "    return in_text"
      ],
      "metadata": {
        "id": "NOk9Fpk8aNTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu"
      ],
      "metadata": {
        "id": "lQQH88jFaNTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validate with test data\n",
        "actual, predicted = list(), list()\n",
        "\n",
        "for key in tqdm(test):\n",
        "    # get actual caption\n",
        "    captions = mapping[key]\n",
        "    # predict the caption for image\n",
        "    y_pred = predict_caption(model, features[key], tokenizer, max_length) \n",
        "    # split into words\n",
        "    actual_captions = [caption.split() for caption in captions]\n",
        "    y_pred = y_pred.split()\n",
        "    # append to the list\n",
        "    actual.append(actual_captions)\n",
        "    predicted.append(y_pred)"
      ],
      "metadata": {
        "id": "6X7w5qzJaNTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calcuate BLEU score\n",
        "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "print(\"BLEU-3: %f\" % corpus_bleu(actual, predicted, weights=(0.2, 0.3, 0.5, 0)))\n",
        "print(\"BLEU-4: %f\" % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "metadata": {
        "id": "z09NqltmaNTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "def generate_caption(image_name):\n",
        "    # load the image\n",
        "    # image_name = \"1001773457_577c3a7d70.jpg\"\n",
        "    image_id = image_name.split('.')[0]\n",
        "    img_path = os.path.join(BASE_DIR, \"Flicker8k_Dataset\", image_name)\n",
        "    image = Image.open(img_path)\n",
        "    captions = mapping[image_id]\n",
        "    print('---------------------Actual---------------------')\n",
        "    for caption in captions:\n",
        "        print(caption)\n",
        "    # predict the caption\n",
        "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
        "    print('--------------------Predicted--------------------')\n",
        "    print(y_pred)\n",
        "    print(\"BLEU-1: %f\" % sentence_bleu(captions, y_pred, weights=(1.0, 0, 0, 0)))\n",
        "    print(\"BLEU-2: %f\" % sentence_bleu(captions, y_pred,  weights=(0.5, 0.5, 0, 0)))\n",
        "    print(\"BLEU-3: %f\" % sentence_bleu(captions, y_pred,  weights=(0.2, 0.3, 0.5, 0)))\n",
        "    print(\"BLEU-4: %f\" % sentence_bleu(captions, y_pred,  weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "    plt.figure()\n",
        "    plt.imshow(image)"
      ],
      "metadata": {
        "id": "1D3RlckeaNTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_rand_cap():\n",
        "    id = np.random.randint(0,len(mapping))\n",
        "    filepaths = os.listdir(BASE_DIR +\"/Flicker8k_Dataset\")\n",
        "    sample_path = filepaths[id] \n",
        "    generate_caption(sample_path)"
      ],
      "metadata": {
        "id": "Ryfv-UZfaNTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_rand_cap()"
      ],
      "metadata": {
        "id": "Dz3WHXfEaNTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eval"
      ],
      "metadata": {
        "id": "ImdS0bgvrXW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import os\n",
        "\n",
        "def calculate_accuracy(model, features, tokenizer, max_length, num_images=None):\n",
        "    total_bleu_score = 0\n",
        "    count = 0\n",
        "    if num_images is None:\n",
        "        num_images = len(features)\n",
        "    else:\n",
        "        num_images = min(num_images, len(features))\n",
        "\n",
        "\n",
        "    for i in range(num_images):\n",
        "        # Load the image\n",
        "        image_name = image_id + \".jpg\"\n",
        "        img_path = os.path.join(BASE_DIR, \"Flicker8k_Dataset\", image_name)\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        # Get the ground truth captions for this image\n",
        "        captions = mapping[image_id]\n",
        "\n",
        "        # Generate the predicted caption\n",
        "        y_pred = predict_caption(model, feature, tokenizer, max_length)\n",
        "\n",
        "        # Calculate the BLEU score for the predicted caption\n",
        "        bleu_score = sentence_bleu(captions, y_pred)\n",
        "\n",
        "        # Print the results for this image\n",
        "        #print(f\"Image {i+1}/{num_images}\")\n",
        "        #print('---------------------Actual---------------------')\n",
        "        #for caption in captions:\n",
        "        #    print(caption)\n",
        "        #print('--------------------Predicted--------------------')\n",
        "       # print(y_pred)\n",
        "        #print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "        #print()\n",
        "\n",
        "        # Add the BLEU score to the total\n",
        "        total_bleu_score += bleu_score\n",
        "\n",
        "    # Calculate the average BLEU score\n",
        "    average_bleu_score = total_bleu_score / num_images\n",
        "\n",
        "    # Print the final results\n",
        "    print(f\"Average BLEU Score: {average_bleu_score:.4f}\")"
      ],
      "metadata": {
        "id": "v0hvGJxarZql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_accuracy(model, features, tokenizer, max_length, num_images=10)"
      ],
      "metadata": {
        "id": "Q_8orWRqrakL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}